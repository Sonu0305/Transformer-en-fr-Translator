{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1067156,"sourceType":"datasetVersion","datasetId":592212}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/language-translation-englishfrench/eng_-french.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-06T07:12:53.448034Z","iopub.execute_input":"2024-09-06T07:12:53.448436Z","iopub.status.idle":"2024-09-06T07:12:53.824987Z","shell.execute_reply.started":"2024-09-06T07:12:53.448396Z","shell.execute_reply":"2024-09-06T07:12:53.823995Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install nltk==3.6.5","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:12:54.831740Z","iopub.execute_input":"2024-09-06T07:12:54.832252Z","iopub.status.idle":"2024-09-06T07:13:10.502298Z","shell.execute_reply.started":"2024-09-06T07:12:54.832213Z","shell.execute_reply":"2024-09-06T07:13:10.501020Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting nltk==3.6.5\n  Downloading nltk-3.6.5-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk==3.6.5) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk==3.6.5) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk==3.6.5) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk==3.6.5) (4.66.4)\nDownloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.6.5 which is incompatible.\ntextblob 0.18.0.post0 requires nltk>=3.8, but you have nltk 3.6.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.6.5\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport string\nimport re\nfrom collections import Counter,OrderedDict\nfrom tqdm.auto import tqdm\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.lm import Vocabulary\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:16:20.466359Z","iopub.execute_input":"2024-09-06T07:16:20.467085Z","iopub.status.idle":"2024-09-06T07:16:20.472793Z","shell.execute_reply.started":"2024-09-06T07:16:20.467045Z","shell.execute_reply":"2024-09-06T07:16:20.471768Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:13:14.712717Z","iopub.execute_input":"2024-09-06T07:13:14.713282Z","iopub.status.idle":"2024-09-06T07:13:14.743360Z","shell.execute_reply.started":"2024-09-06T07:13:14.713234Z","shell.execute_reply":"2024-09-06T07:13:14.742183Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/language-translation-englishfrench/eng_-french.csv')\ndf.columns = [\"eng\",\"fre\"]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:13:14.745374Z","iopub.execute_input":"2024-09-06T07:13:14.745690Z","iopub.status.idle":"2024-09-06T07:13:15.280273Z","shell.execute_reply.started":"2024-09-06T07:13:14.745658Z","shell.execute_reply":"2024-09-06T07:13:15.279339Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"    eng         fre\n0   Hi.      Salut!\n1  Run!     Cours !\n2  Run!    Courez !\n3  Who?       Qui ?\n4  Wow!  Ça alors !","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>fre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi.</td>\n      <td>Salut!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Who?</td>\n      <td>Qui ?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(df.isna().sum())\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:13:15.281609Z","iopub.execute_input":"2024-09-06T07:13:15.282002Z","iopub.status.idle":"2024-09-06T07:13:15.355349Z","shell.execute_reply.started":"2024-09-06T07:13:15.281958Z","shell.execute_reply":"2024-09-06T07:13:15.354427Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"eng    0\nfre    0\ndtype: int64\neng    0\nfre    0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"valid_puncs = \"-,'!?.\"\ninvalid_puncs = string.punctuation         #returns all sets of punctuations\n\nfor punc in valid_puncs:\n    invalid_puncs = invalid_puncs.replace(punc,\"\")\ninvalid_puncs_re = re.compile(\"[\"+ invalid_puncs +\"]\")   #expression to match any invalid punctuations \n\ndef text_sanitization(text: str) -> str:\n    text = invalid_puncs_re.sub(\"\", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:13:15.356518Z","iopub.execute_input":"2024-09-06T07:13:15.356821Z","iopub.status.idle":"2024-09-06T07:13:15.362632Z","shell.execute_reply.started":"2024-09-06T07:13:15.356789Z","shell.execute_reply":"2024-09-06T07:13:15.361760Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df[\"eng\"] = df[\"eng\"].apply(text_sanitization)\ndf[\"fre\"] = df[\"fre\"].apply(text_sanitization)\ndf.tail(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:13:15.363689Z","iopub.execute_input":"2024-09-06T07:13:15.363967Z","iopub.status.idle":"2024-09-06T07:13:17.259853Z","shell.execute_reply.started":"2024-09-06T07:13:15.363936Z","shell.execute_reply":"2024-09-06T07:13:17.258940Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                      eng  \\\n175616  Top-down economics never works, said Obama. Th...   \n175617  A carbon footprint is the amount of carbon dio...   \n175618  Death is something that we're often discourage...   \n175619  Since there are usually multiple websites on a...   \n175620  If someone who doesn't know your background sa...   \n\n                                                      fre  \n175616  « L'économie en partant du haut vers le bas, ç...  \n175617  Une empreinte carbone est la somme de pollutio...  \n175618  La mort est une chose qu'on nous décourage sou...  \n175619  Puisqu'il y a de multiples sites web sur chaqu...  \n175620  Si quelqu'un qui ne connaît pas vos antécédent...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>fre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>175616</th>\n      <td>Top-down economics never works, said Obama. Th...</td>\n      <td>« L'économie en partant du haut vers le bas, ç...</td>\n    </tr>\n    <tr>\n      <th>175617</th>\n      <td>A carbon footprint is the amount of carbon dio...</td>\n      <td>Une empreinte carbone est la somme de pollutio...</td>\n    </tr>\n    <tr>\n      <th>175618</th>\n      <td>Death is something that we're often discourage...</td>\n      <td>La mort est une chose qu'on nous décourage sou...</td>\n    </tr>\n    <tr>\n      <th>175619</th>\n      <td>Since there are usually multiple websites on a...</td>\n      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n    </tr>\n    <tr>\n      <th>175620</th>\n      <td>If someone who doesn't know your background sa...</td>\n      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"en_words = Counter()\nfor sentence in tqdm(df[\"eng\"]):\n    en_words.update(word_tokenize(sentence, language='english'))\nen_words = OrderedDict(sorted(en_words.items(), key=lambda x:-x[1]))\n\nfr_words = Counter()\nfor sentence in tqdm(df[\"fre\"]):\n    fr_words.update(word_tokenize(sentence, language='french'))\nfr_words = OrderedDict(sorted(fr_words.items(), key=lambda x:-x[1]))\n\nprint(f\"No. of english words: {len(en_words)}\")\nprint(f\"No. of french words: {len(fr_words)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:13:17.260850Z","iopub.execute_input":"2024-09-06T07:13:17.261129Z","iopub.status.idle":"2024-09-06T07:14:09.964409Z","shell.execute_reply.started":"2024-09-06T07:13:17.261099Z","shell.execute_reply":"2024-09-06T07:14:09.963491Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/175621 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a0ce1618fd4da788513e858f3bf612"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/175621 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"166c5aed645e42d8bb197a2c3f3d62ad"}},"metadata":{}},{"name":"stdout","text":"No. of english words: 16220\nNo. of french words: 31537\n","output_type":"stream"}]},{"cell_type":"code","source":"unk_token = '<unk>'  # Unknown word token\nsos_token = '<sos>'  # Start of sentence token\neos_token = '<eos>'  # End of sentence token\npad_token = '<pad>'  #padding token\n\nen_words.update({unk_token: 0, eos_token: 1, pad_token: 2})\nfr_words.update({unk_token: 0, sos_token: 1, eos_token: 2, pad_token: 3})\n\nen_vocab = Vocabulary(list(en_words.keys()), unk_label=unk_token)\nfr_vocab = Vocabulary(list(fr_words.keys()), unk_label=unk_token)\n\nen_token_to_index = {token: index for index, token in enumerate(en_vocab)} #pad_token 16222\nfr_token_to_index = {token: index for index, token in enumerate(fr_vocab)} #pad_token 31540\nprint(len((en_token_to_index)))\nprint(len((fr_token_to_index)))\nprint(en_token_to_index['<pad>'],fr_token_to_index['<pad>'])","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:14:09.965782Z","iopub.execute_input":"2024-09-06T07:14:09.966447Z","iopub.status.idle":"2024-09-06T07:14:10.065838Z","shell.execute_reply.started":"2024-09-06T07:14:09.966390Z","shell.execute_reply":"2024-09-06T07:14:10.064956Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"16223\n31541\n16222 31540\n","output_type":"stream"}]},{"cell_type":"code","source":"def en_tokenize(text: str, append_eos=True):\n    words = word_tokenize(text, language='english')\n    if append_eos:\n        words.append(eos_token)\n    return [en_token_to_index.get(word, en_token_to_index[unk_token]) for word in words]\n\ndef fr_tokenize(text: str, append_eos=True):\n    words = [sos_token] + word_tokenize(text, language='french')\n    if append_eos:\n        words.append(eos_token)\n    return [fr_token_to_index.get(word, fr_token_to_index[unk_token]) for word in words]","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:14:10.068693Z","iopub.execute_input":"2024-09-06T07:14:10.068993Z","iopub.status.idle":"2024-09-06T07:14:10.075116Z","shell.execute_reply.started":"2024-09-06T07:14:10.068961Z","shell.execute_reply":"2024-09-06T07:14:10.074252Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df[\"eng\"], df[\"fre\"], train_size = 0.8)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:14:10.076360Z","iopub.execute_input":"2024-09-06T07:14:10.077146Z","iopub.status.idle":"2024-09-06T07:14:10.117622Z","shell.execute_reply.started":"2024-09-06T07:14:10.077103Z","shell.execute_reply":"2024-09-06T07:14:10.116733Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X_train_seqs = [en_tokenize(x) for x in X_train]\nX_test_seqs = [en_tokenize(x) for x in X_test]\n\ny_train_seqs = [fr_tokenize(y) for y in y_train]\ny_test_seqs = [fr_tokenize(y) for y in y_test]","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:14:10.118775Z","iopub.execute_input":"2024-09-06T07:14:10.119132Z","iopub.status.idle":"2024-09-06T07:15:01.775824Z","shell.execute_reply.started":"2024-09-06T07:14:10.119090Z","shell.execute_reply":"2024-09-06T07:15:01.774996Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(en_tokenize(\"Hi this is Swayam.\"))\nprint(en_tokenize(\"lets talk about project x\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:15:01.776920Z","iopub.execute_input":"2024-09-06T07:15:01.777262Z","iopub.status.idle":"2024-09-06T07:15:01.782962Z","shell.execute_reply.started":"2024-09-06T07:15:01.777221Z","shell.execute_reply":"2024-09-06T07:15:01.781977Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[4283, 23, 8, 16223, 0, 16221]\n[4554, 138, 55, 1217, 16223, 16221]\n","output_type":"stream"}]},{"cell_type":"code","source":"class SeqDataset(Dataset):\n    def __init__(self,en_seqs,fr_seqs):\n        super().__init__()\n        self.en_seqs = en_seqs\n        self.fr_seqs = fr_seqs\n        \n    def __len__(self):\n        return len(self.en_seqs)\n    \n    def __getitem__(self, index):\n        encoder_input = torch.tensor(self.en_seqs[index]).long()\n        decoder_input = torch.tensor(self.fr_seqs[index][:-1]).long()     \n        answer = torch.tensor(self.fr_seqs[index][1:]).long()     #right shift\n        return encoder_input, decoder_input, answer\n    \ndef collate_fn(batch):\n    encoder_ip = []\n    decoder_ip = []\n    answer = []\n    for e, d, a in batch:\n        encoder_ip.append(e)\n        decoder_ip.append(d)\n        answer.append(a)\n        \n    encoder_ip = nn.utils.rnn.pad_sequence(encoder_ip, batch_first= True, padding_value= 16222)\n    decoder_ip = nn.utils.rnn.pad_sequence(decoder_ip, batch_first= True, padding_value= 31540)\n    answer = nn.utils.rnn.pad_sequence(answer, batch_first= True, padding_value= 31540)\n    return encoder_ip, decoder_ip, answer","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:15:01.784109Z","iopub.execute_input":"2024-09-06T07:15:01.784442Z","iopub.status.idle":"2024-09-06T07:15:01.797773Z","shell.execute_reply.started":"2024-09-06T07:15:01.784410Z","shell.execute_reply":"2024-09-06T07:15:01.796978Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"trainset = SeqDataset(X_train_seqs, y_train_seqs)\ntestset = SeqDataset(X_test_seqs, y_test_seqs)\nlen(trainset), len(testset)\n\ntrainloader = DataLoader(trainset, batch_size= 8, shuffle= True, collate_fn= collate_fn)\ntestLoader = DataLoader(testset, batch_size= 8, shuffle= False, collate_fn= collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:15:01.798803Z","iopub.execute_input":"2024-09-06T07:15:01.799090Z","iopub.status.idle":"2024-09-06T07:15:01.812233Z","shell.execute_reply.started":"2024-09-06T07:15:01.799059Z","shell.execute_reply":"2024-09-06T07:15:01.811375Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"ip_dim = len(en_vocab)\nop_dim = len(fr_vocab)\nd_model = 512       #default value 512\nnum_enc_layers = 1  #default value 6\nnum_dec_layers = 1  #default value 6\nn_heads= 8          #default value 8\ndropout_val = 0.1   #default value 0.1\nsrc_pad_idx= 16222\ntgt_pad_idx= 31540 ","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:15:01.813297Z","iopub.execute_input":"2024-09-06T07:15:01.813582Z","iopub.status.idle":"2024-09-06T07:15:01.829361Z","shell.execute_reply.started":"2024-09-06T07:15:01.813551Z","shell.execute_reply":"2024-09-06T07:15:01.828405Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class TransformersTranslation(nn.Module):\n    def __init__(self, ip_dim, op_dim, d_model, num_enc_layers, num_dec_layers, n_heads, dropout_val, src_pad_idx, tgt_pad_idx):\n        super(TransformersTranslation, self).__init__()\n        self.embeddings_ip = nn.Embedding(ip_dim, d_model, padding_idx=src_pad_idx)\n        self.embeddings_op = nn.Embedding(op_dim, d_model, padding_idx=tgt_pad_idx)\n        \n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=n_heads,\n            num_encoder_layers=num_enc_layers,\n            num_decoder_layers=num_dec_layers,\n            dropout=dropout_val,\n            batch_first=True\n        )\n        \n        self.fc_out = nn.Linear(d_model, op_dim)\n        self.dropout = nn.Dropout(dropout_val)\n        \n        self.layer_norm_src = nn.LayerNorm(d_model)\n        self.layer_norm_tgt = nn.LayerNorm(d_model)\n        \n        self.src_pad_idx = src_pad_idx\n        self.tgt_pad_idx = tgt_pad_idx\n        \n    def create_padding_mask(self, seq, pad_idx):\n        # seq shape: [batch_size, seq_len]\n        # pad_idx is the index to be masked\n        return (seq == pad_idx)  # Shape: [batch_size, seq_len]\n\n    def create_look_ahead_mask(self, size):\n        mask = torch.triu(torch.ones(size, size, dtype=torch.bool), diagonal=1)\n        return mask.masked_fill(mask == 1, float('-inf'))  # Shape: [seq_len, seq_len]\n        \n    def forward(self, src, tgt):\n        src = self.embeddings_ip(src)  # Shape: [batch_size, src_seq_len, d_model]\n        tgt = self.embeddings_op(tgt)  # Shape: [batch_size, tgt_seq_len, d_model]\n        \n        src = self.dropout(src)\n        tgt = self.dropout(tgt)\n        \n        src = self.layer_norm_src(src)\n        src = src.to(device)\n        tgt = self.layer_norm_tgt(tgt)\n        tgt = tgt.to(device)\n        \n        src_padding_mask = self.create_padding_mask(src[:, :, 0], self.src_pad_idx)  # Shape: [batch_size, src_seq_len]\n        src_padding_mask = src_padding_mask.to(device)\n        tgt_padding_mask = self.create_padding_mask(tgt[:, :, 0], self.tgt_pad_idx)  # Shape: [batch_size, tgt_seq_len]\n        tgt_padding_mask = tgt_padding_mask.to(device)\n        tgt_look_ahead_mask = self.create_look_ahead_mask(tgt.size(1))  # Shape: [tgt_seq_len, tgt_seq_len]\n        tgt_look_ahead_mask = tgt_look_ahead_mask.to(device)\n        \n        output = self.transformer(\n            src, tgt,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask,\n            memory_key_padding_mask=src_padding_mask,\n            tgt_mask=tgt_look_ahead_mask\n        )\n        \n        output = self.fc_out(output)\n        output = output.to(device)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:15:01.830650Z","iopub.execute_input":"2024-09-06T07:15:01.830992Z","iopub.status.idle":"2024-09-06T07:15:01.845396Z","shell.execute_reply.started":"2024-09-06T07:15:01.830949Z","shell.execute_reply":"2024-09-06T07:15:01.844525Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = TransformersTranslation(ip_dim, op_dim, d_model, num_enc_layers, num_dec_layers, n_heads, dropout_val, src_pad_idx, tgt_pad_idx).to(device)\n# model","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:15:01.846566Z","iopub.execute_input":"2024-09-06T07:15:01.847281Z","iopub.status.idle":"2024-09-06T07:15:02.606836Z","shell.execute_reply.started":"2024-09-06T07:15:01.847184Z","shell.execute_reply":"2024-09-06T07:15:02.605997Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, dataloader, loss_fn, optimizer, device):\n    model.train()  \n    epoch_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n\n    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n        src, tgt, tgt_y = batch\n        src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n        \n        optimizer.zero_grad()\n        \n        output = model(src, tgt)\n        output = output.to(device)\n        \n        loss = loss_fn(output.view(-1, output.size(-1)), tgt_y.contiguous().view(-1))\n        loss = loss.to(device)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n        preds = output.argmax(dim=-1)  \n        correct_predictions += (preds == tgt_y).sum().item()\n        total_predictions += tgt_y.numel()\n    \n    avg_epoch_loss = epoch_loss / len(dataloader)\n    accuracy = correct_predictions / total_predictions\n    return avg_epoch_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-08-31T05:49:34.895187Z","iopub.execute_input":"2024-08-31T05:49:34.895492Z","iopub.status.idle":"2024-08-31T05:49:34.904456Z","shell.execute_reply.started":"2024-08-31T05:49:34.895458Z","shell.execute_reply":"2024-08-31T05:49:34.903173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_epoch(model, dataloader, loss_fn, device):\n    model.eval()  \n    epoch_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n    with torch.no_grad():  \n        for batch in tqdm(dataloader, desc=\"Testing\", leave=False):\n            src, tgt, tgt_y = batch\n            src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n            \n            output = model(src, tgt)\n            output = output.to(device)\n            \n            loss = loss_fn(output.view(-1, output.size(-1)), tgt_y.contiguous().view(-1))\n            loss = loss.to(device)\n            \n            epoch_loss += loss.item()\n            \n            preds = output.argmax(dim=-1)  \n            correct_predictions += (preds == tgt_y).sum().item()\n            total_predictions += tgt_y.numel()\n    \n    avg_epoch_loss = epoch_loss / len(dataloader)\n    accuracy = correct_predictions / total_predictions\n    return avg_epoch_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-08-31T05:49:34.905653Z","iopub.execute_input":"2024-08-31T05:49:34.906009Z","iopub.status.idle":"2024-08-31T05:49:34.920700Z","shell.execute_reply.started":"2024-08-31T05:49:34.905972Z","shell.execute_reply":"2024-08-31T05:49:34.919796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(model, epoch, train_acc, train_loss, val_acc, val_loss, directory='/kaggle/working/'):\n    filename = (f\"ep-{epoch+1:03d}-train_acc-{train_acc:.4f}-train_loss-{train_loss:.4f}-\"\n                f\"val_acc-{val_acc:.4f}-val_loss-{val_loss:.4f}.pth\")\n    filepath = os.path.join(directory, filename)\n    \n    torch.save(model.state_dict(), filepath)\n    print(f\"Model saved to {filepath}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss(ignore_index= tgt_pad_idx) \nloss_fn = loss_fn.to(device)\noptimizer = optim.AdamW(model.parameters(), lr= 3e-4, weight_decay= 1e-2)\nEPOCHS= 90\n\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_epoch(model, trainloader, loss_fn, optimizer, device)\n    test_loss, test_accuracy = test_epoch(model, testLoader, loss_fn, device)\n    \n    print(f'Epoch {epoch+1}/{EPOCHS}')\n    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n    if (epoch + 1) % 5 == 0:\n        save_model(model, epoch, train_accuracy, train_loss, test_accuracy, test_loss)\n        print(f'Model saved at {epoch+1}')","metadata":{"execution":{"iopub.status.busy":"2024-08-31T05:49:34.921893Z","iopub.execute_input":"2024-08-31T05:49:34.922229Z","iopub.status.idle":"2024-08-31T07:10:07.766831Z","shell.execute_reply.started":"2024-08-31T05:49:34.922191Z","shell.execute_reply":"2024-08-31T07:10:07.765229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/85-th-epoch-translation-model-saved/ep-085-train_acc-0.5526-train_loss-0.6162-val_acc-0.4782-val_loss-1.6575.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:15:02.607951Z","iopub.execute_input":"2024-09-06T07:15:02.608287Z","iopub.status.idle":"2024-09-06T07:15:04.456312Z","shell.execute_reply.started":"2024-09-06T07:15:02.608253Z","shell.execute_reply":"2024-09-06T07:15:04.455277Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2942264386.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/85-th-epoch-translation-model-saved/ep-085-train_acc-0.5526-train_loss-0.6162-val_acc-0.4782-val_loss-1.6575.pth'))\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"def inference(model, en_sentence):\n    model.eval()\n    \n    encode_seq = torch.tensor([en_tokenize(en_sentence, append_eos=False)]).to(device)\n    \n    decode_seq = torch.tensor([[fr_token_to_index[sos_token]]]).to(device)\n    \n    for _ in range(10):  \n        with torch.no_grad():\n            \n            out_logits = model(encode_seq, decode_seq)\n            \n            out_logits = out_logits[:, -1, :]  \n            out_seqs = torch.argmax(out_logits, dim=-1)  \n            \n            next_token = out_seqs.item()\n            \n            decode_seq = torch.cat([decode_seq, torch.tensor([[next_token]], device=device)], dim=1)\n            \n            if next_token == fr_token_to_index[eos_token]:\n                break\n    \n    tokens = [word for token_id in decode_seq[0].tolist() for word, idx in fr_token_to_index.items() if idx == token_id]\n    \n    result = ' '.join(tokens)\n    \n    return result.replace(f' {eos_token}', '')\n\n# Prompt user for input\ndef main():\n    while True:\n        user_input = input(\"Enter an English sentence for translation (or type 'exit' to quit): \")\n        if user_input.lower() == 'exit':\n            break\n        translated_sentence = inference(model, user_input)\n        print(f\"French translation: {translated_sentence}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:32:44.213146Z","iopub.execute_input":"2024-09-06T07:32:44.213530Z","iopub.status.idle":"2024-09-06T07:35:17.301179Z","shell.execute_reply.started":"2024-09-06T07:32:44.213497Z","shell.execute_reply":"2024-09-06T07:35:17.300339Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  answer this morning\n"},{"name":"stdout","text":"French translation: <sos> ce ce ce ce matin ce ce ce ce ce\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  answer today morning\n"},{"name":"stdout","text":"French translation: <sos> aujourd'hui ne aujourd'hui quelques aujourd'hui , aujourd'hui , aujourd'hui ,\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  answer in morning\n"},{"name":"stdout","text":"French translation: <sos> Comme ce matin à Comme il Comme à Comme aujourd'hui\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  answer by today\n"},{"name":"stdout","text":"French translation: <sos> aujourd'hui ne aujourd'hui , aujourd'hui , aujourd'hui , aujourd'hui ,\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  roaming in garden\n"},{"name":"stdout","text":"French translation: <sos> Je temps à ce que tu arriva à temps dans\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  time\n"},{"name":"stdout","text":"French translation: <sos> temps l'heure temps l'heure l'heure l'heure l'heure l'heure l'heure l'heure\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  in today\n"},{"name":"stdout","text":"French translation: <sos> aujourd'hui , aujourd'hui , aujourd'hui , aujourd'hui , aujourd'hui ,\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  in\n"},{"name":"stdout","text":"French translation: <sos> dans deux dans dans dans un dans temps pièce pièce\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  on foot\n"},{"name":"stdout","text":"French translation: <sos> Le temps sur le départ à des sur le pied\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an English sentence for translation (or type 'exit' to quit):  exit\n"}]},{"cell_type":"code","source":"def inference(model, en_sentence):\n    model.eval()  # Set the model to evaluation mode\n    \n    # Tokenize and prepare the input sequence\n    encode_seq = torch.tensor([en_tokenize(en_sentence, append_eos=False)]).to(device)\n    \n    # Start with the start-of-sequence token\n    decode_seq = torch.tensor([[fr_token_to_index[sos_token]]]).to(device)\n    \n    result_tokens = []  # List to accumulate tokens\n    \n    for _ in range(200):  # Limit to 200 tokens for the output\n        with torch.no_grad():\n            out_logits = model(encode_seq, decode_seq)  # Get model output\n            out_logits = out_logits[:, -1, :]  # Shape: [batch_size, vocab_size]\n            \n            # Get the predicted token IDs\n            out_seqs = torch.argmax(out_logits, dim=-1)  # Shape: [batch_size]\n            \n            # Extract the predicted token\n            next_token = out_seqs.item()  # Get the single token ID\n            \n            # Append the predicted token to the result tokens list\n            result_tokens.append(next_token)\n            \n            # Check for end-of-sequence token\n            if next_token == fr_token_to_index[eos_token]:\n                break\n            \n            # Update the decode sequence with the predicted token\n            decode_seq = torch.cat([decode_seq, torch.tensor([[next_token]], device=device)], dim=1)\n    \n    # Convert token IDs to tokens manually using index-to-string mapping\n    tokens = [list(fr_token_to_index.keys())[list(fr_token_to_index.values()).index(token_id)] for token_id in result_tokens]\n    \n    # Join tokens to form the final translation string\n    result = ' '.join(tokens)\n    \n    # Return the result, removing any trailing <eos> token\n    return result.replace(f' {eos_token}', '')\n\n# Testing the function\ndata = list(zip(X_test, y_test))  # Assuming X_test and y_test are the test datasets\nrandom.shuffle(data)\n\nfor x, y in data[:10]:\n    result = inference(model, x)\n    print(\"English:\", x)\n    print(\"Model Translation:\", result)\n    print(\"Actual French:\", y)\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-06T07:41:29.728678Z","iopub.execute_input":"2024-09-06T07:41:29.729485Z","iopub.status.idle":"2024-09-06T07:41:30.289379Z","shell.execute_reply.started":"2024-09-06T07:41:29.729442Z","shell.execute_reply":"2024-09-06T07:41:30.288450Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"English: It's sunny, but the water is cold.\nModel Translation: Il y a des choses , mais il y a des faut .\nActual French: Il y a du soleil, mais l'eau est froide.\n\nEnglish: Good friends are like stars. You don't always see them, but you know they are always there.\nModel Translation: On ne sait pas les amis , mais il y a tout le temps .\nActual French: Les bons amis sont comme les étoiles on ne les voit pas toujours, mais on sait qu'ils sont toujours là.\n\nEnglish: I think you lied to me.\nModel Translation: Je pense que tu me voies . Je pense que tu m'as menti .\nActual French: Je pense que tu m'as menti.\n\nEnglish: What did you say?\nModel Translation: Qu ' a dit ?\nActual French: Qu'avez-vous dit ?\n\nEnglish: The system is rigged.\nModel Translation: Le .\nActual French: Le système est truqué.\n\nEnglish: I always enjoy listening to classical music when I have some free time.\nModel Translation: J'ai toujours aimé disposer d'un petit peu de temps lorsque j'aime à Lorsque j'ai un peu de temps en temps .\nActual French: J'ai toujours aimé écouter de la musique classique quand j'ai un peu de temps libre.\n\nEnglish: I'll carry this suitcase for you.\nModel Translation: Je vais porter cette valise pour vous . Je vais vous ceci .\nActual French: Je vais porter cette valise pour vous.\n\nEnglish: He passed his English examination.\nModel Translation: Il passa son anglais .\nActual French: Il a réussi à son examen d'anglais.\n\nEnglish: Fasten your seat belt.\nModel Translation: Tom a monté ta part .\nActual French: Attachez votre ceinture.\n\nEnglish: I saw the car hit a man.\nModel Translation: J'ai vu la voiture que j'ai vu un homme hier .\nActual French: J'ai vu la voiture heurter un homme.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}